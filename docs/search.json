[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Work",
    "section": "",
    "text": "Mount Whitney, California Land Cover\n\n\n\nMEDS\n\n\nGeospatial\n\n\nPython\n\n\n\nExploring land cover types and their dominance on the landscape around Mount Whitney, California\n\n\n\nMelissa Widas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouston, Texas Power Outages\n\n\n\nMEDS\n\n\nGeospatial\n\n\nR\n\n\n\nIdentifying homes affected by severe winter storms in Houston, Texas energy crisis\n\n\n\nMelissa Widas\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoil Temperature Analysis at Toolik Field Station, AK\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\n\nExploring the relationship between soil temperatures, air temperatures, and black carbon levels at Toolik Field Station, AK\n\n\n\nMelissa Widas\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization Accessibility\n\n\n\nQuarto\n\n\nMEDS\n\n\nEthics\n\n\n\nExamining ways data scientists can create data visualizations that are more widely accessable\n\n\n\nMelissa Widas\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "",
    "text": "Soil is all around us. Underneath our homes, creating landscapes and environments we like to recreate in and also a vital carbon sink. Soil, and especially permafrost soils, play a key role in storing greenhouse gases and releasing greenhouse gases as they warm. A permafrost soil is defined as a frozen soil that is at or below zero degrees Celcius, and has been for at least two years (Snow and Ice Data Center, n.d.). Climate change and climate change modeling varies widely on predicting the fate and impacts of permafrost soils (Zhu et al. 2019), but what are some of the main drivers behind permafrost melt?\nUltimately, the main driver behind permafrost melt is temperature. And the obvious thought is of course a warmer temperature will lead to warmer soil temperatures which will ultimately warm permafrost soils to the point where they are no longer frozen. As a broad overview this thought process works, however there are other drivers that determine soil temperature. Soil properties such as soil organic carbon, soil texture, bulk density, and soil moisture all impact how soils respond to temperature. However, it has been shown that soil organic carbon can be the dominant determinant of soil temperature in permafrost soils (Zhu et al. 2019). One way soils build up soil organic carbon is through the deposition of black carbon (Kopecký et al. 2021).\nBlack carbon is predominantly present as products of incomplete combustion (Environmental Protection Agency 2016) . By incomplete combustion we are talking about soot from wildfires, fossil fuels, and biofuels. As temperature regimes continue to change globally the cadence of black carbon from inputs like wildfires can also be expected to shift.\nTherefore a question that comes to mind with these findings are if air temperature and black carbon levels both affect soil temperature, can they predict soil temperature and in the process is air temperature and black carbon interacting with each other?\nTo explore this question further data from Toolik Field Station, AK will be explored. The Toolik Field Station is a research station operated and managed by the Institute of Arctic Biology at the University of Alaska Fairbanks with cooperative agreement support from the Division of Polar Programs, Directorate for Geosciences at the National Science Foundation (NSF)."
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#background",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#background",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "",
    "text": "Soil is all around us. Underneath our homes, creating landscapes and environments we like to recreate in and also a vital carbon sink. Soil, and especially permafrost soils, play a key role in storing greenhouse gases and releasing greenhouse gases as they warm. A permafrost soil is defined as a frozen soil that is at or below zero degrees Celcius, and has been for at least two years (Snow and Ice Data Center, n.d.). Climate change and climate change modeling varies widely on predicting the fate and impacts of permafrost soils (Zhu et al. 2019), but what are some of the main drivers behind permafrost melt?\nUltimately, the main driver behind permafrost melt is temperature. And the obvious thought is of course a warmer temperature will lead to warmer soil temperatures which will ultimately warm permafrost soils to the point where they are no longer frozen. As a broad overview this thought process works, however there are other drivers that determine soil temperature. Soil properties such as soil organic carbon, soil texture, bulk density, and soil moisture all impact how soils respond to temperature. However, it has been shown that soil organic carbon can be the dominant determinant of soil temperature in permafrost soils (Zhu et al. 2019). One way soils build up soil organic carbon is through the deposition of black carbon (Kopecký et al. 2021).\nBlack carbon is predominantly present as products of incomplete combustion (Environmental Protection Agency 2016) . By incomplete combustion we are talking about soot from wildfires, fossil fuels, and biofuels. As temperature regimes continue to change globally the cadence of black carbon from inputs like wildfires can also be expected to shift.\nTherefore a question that comes to mind with these findings are if air temperature and black carbon levels both affect soil temperature, can they predict soil temperature and in the process is air temperature and black carbon interacting with each other?\nTo explore this question further data from Toolik Field Station, AK will be explored. The Toolik Field Station is a research station operated and managed by the Institute of Arctic Biology at the University of Alaska Fairbanks with cooperative agreement support from the Division of Polar Programs, Directorate for Geosciences at the National Science Foundation (NSF)."
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#data",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#data",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "Data",
    "text": "Data\n\n\nCode\n# import necessary libraries\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(sjPlot)\n\n\nSoil Temperature\nSoil Temperature data was accessed from the publicly available Meteorological Data Query from Lake Toolik Field Station (Environmental Data Center Team. 2023). The soil temperature marker utilized for this analysis was the soil temperature for the first 150 centimeters of the soil profile from 2019 to 2022. All measurements were measured in degrees Celsius, and recorded every three hours. For this analysis measurements recorded at the 300 hour mark each week were utilized.\n\n\nCode\n# load data that has soil temperature in degrees Celcius for first 150 cm\nsoil &lt;- read_csv(here(\"blog-posts\", \"2023-12-10-permafrost-black-carbon\", \"data\",\"3-hour_data.csv\"))\n\n# create updated soil temperature df\nsoil_2 &lt;- soil %&gt;% \n  # choose all values recorded at hour 300\n  filter(hour == \"300\") %&gt;%\n  # remove hour column\n  subset(select = -hour) %&gt;%\n  # rename column for readability\n  rename(soil_temp = soil1_150cm) \n\n\nAir Temperature\nAir Temperature data was accessed from the publicly available Meteorological Data Query from Lake Toolik Field Station (Environmental Data Center Team. 2023). The air temperature used in this analysis was the air temperature max recorded for each period of one week from 2019 to 2022 in degrees Celsius.\n\n\nCode\n# load data that contains air temperature in degrees celcius for Toolik Field Station, Alaska\nair_temp &lt;- read_csv(here(\"blog-posts\", \"2023-12-10-permafrost-black-carbon\", \"data\", \"24-hour_data.csv\"))\n\n# create updated air temperature df\nair_temp_updated &lt;- air_temp %&gt;% \n  dplyr::select(air_temp_max_3m, date) %&gt;% # select columns to keep\n  rename(air_temp = air_temp_max_3m) # rename air_temp for readability\n\n\nBlack Carbon\nBlack Carbon data was accessed from a Purple Air Sensor. The Purple Air Sensor was installed in 2019 in collaboration with the Atmospheric Chemistry and Climate Lab at the University of Alaska Fairbanks run by Dr. Jingqiu Mao. To quantify black carbon Particulate Matter 2.5 ug/m3 was used. These data are recorded weekly.\n\n\nCode\n# load data that contains pm2.5 which will be used as a representation of black carbon\nblack_carbon &lt;- read_csv(here(\"blog-posts\", \"2023-12-10-permafrost-black-carbon\", \"data\", \"us-epa-pm25-aqi.csv\"))\n\n# black carbon date information is stored as POSIXct so switch to date type\nblack_carbon$DateTime &lt;- as.Date(black_carbon$DateTime) \n\n# create updated black carbon dataframe\nblack_carbon_2 &lt;- black_carbon %&gt;% \n  # rename date column to match other df's\n  rename(date = DateTime) %&gt;%  \n  # average the two field station measurements\n  mutate(b_c = (`Toolik Field Station A`+`Toolik Field Station B`)/2) %&gt;% \n  # b_c is not normally distributed so take log to correct for this\n  mutate(log_b_c = (log(b_c)+1)) %&gt;% \n  # remove unecessary columns\n  subset(select = -c(Average,`Toolik Field Station A`, `Toolik Field Station B`))  \n  \n# change -Inf values created by log function to 0 to allow regressions to run\nblack_carbon_2$log_b_c[black_carbon_2$log_b_c == -Inf] &lt;- 0 \n\n\n\n\nCode\n# create new df containing black carbon and soil temp data\ncombined_bc_st &lt;- left_join(black_carbon_2, soil_2, by = 'date')\n\n# create new df with black carbon, soil temp, and air temp data\nfull &lt;- left_join(combined_bc_st, air_temp_updated, by = 'date') %&gt;% \n  drop_na() # remove na values"
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#exploratory-analysis",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#exploratory-analysis",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThe following figures show the shape and distribution of these data and help to determine that a linear regression is appropriate.\n\n\nCode\n# plot relationship between air temperature and soil temperature\nggplot(data = full, aes(x = air_temp, y = soil_temp)) +\n  geom_point() +\n  theme_minimal() +\n  geom_smooth(method = lm) +\n  labs(x = \"Air Temperature degrees Celsius\", y = \"Soil Temperature degrees Celsius\")\n\n\n\n\n\n\n\nCode\n# plot relationship between black carbon and soil temp\nggplot(data = full, aes(x = b_c, y = soil_temp)) +\n  geom_point() +\n  theme_minimal() +\n  geom_smooth(method = lm) +\n  labs(x = \"black carbon (ug/m3)\", y = \"Soil Temperature degrees Celsius\")\n\n\n\n\n\nCode\n# clustering of data, use a histogram to examine the distribution\nhist(full$b_c, xlab = 'black carbon (ug/m3)', main = 'Distribution of Black Carbon levels')\n\n\n\n\n\nHowever, something to note is that our black carbon data is not distributed normally which could lead to bias in our results. A log transformation will be applied to correct for this skewed distribution. After the log transformation is applies to black carbon our distribution is more normally distributed.\n\n\nCode\n# re-examine black carbon with a log transformation applied to see if relationship improves\nggplot(data = full, aes(x = log_b_c, y = soil_temp)) +\n  geom_point() +\n  theme_minimal() +\n  geom_smooth(method = lm) +\n  labs(x = \"log(black carbon)\", y = \"Soil Temperature degrees Celsius\")\n\n\n\n\n\nCode\n# examine distribution of log transformed black carbon levels\nhist(full$log_b_c, xlab = 'log(black carbon)', main = 'Distribution of Black Carbon levels')"
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#analysis",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#analysis",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "Analysis",
    "text": "Analysis\nA multi-linear regression model will allow us to answer our question. To explore if black carbon and air temperature affect soil temperature and if black carbon and air temperature have an effect on each other, I used the following interaction model equation:\n\\[ soiltemp_i=\\beta_{0}+\\beta_{1} \\cdot airtemp_i + \\beta_{2} \\cdot log(black carbon)_i + \\beta_{3} \\cdot airtemp_i \\cdot log(blackcarbon)_i + \\varepsilon_i \\]\nwhich resulted in the following inferences:\n\n\nCode\n# run linear regression on soil temp using air temp and black carbon as well as an interaction between black carbon and air temperature\npermafrost_mod_3 &lt;- lm(soil_temp ~ air_temp + log_b_c + air_temp:log_b_c, data = full)\npermafrost_mod_3_summ &lt;- summary(permafrost_mod_3)\n\n# create a table with results displayed from interaction model\npermafrost_table &lt;- permafrost_mod_3 %&gt;%\n  tab_model()\n\npermafrost_table\n\n\n\n\n\n \nsoil temp\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.82\n-1.61 – -0.03\n0.041\n\n\nair temp\n0.06\n-0.01 – 0.12\n0.079\n\n\nlog b c\n-0.36\n-0.70 – -0.02\n0.036\n\n\nair temp × log b c\n0.01\n-0.02 – 0.04\n0.430\n\n\nObservations\n150\n\n\nR2 / R2 adjusted\n0.196 / 0.179\n\n\n\n\n\n\n\nThe interaction model is able to provide this analysis with some guiding results through hypothesis testing. Our interaction model provides us with three hypotheses:\nHypothesis 1\nOur first null hypothesis is that there is no effect of air temperature in degrees Celsius on soil temperature in degrees Celsius. The alternative hypothesis being that there is an affect of air temperature in degrees Celsius on soil temperature in degrees Celsius. We find a statistically significant effect whereby increases in air temperature degrees Celsius increase soil temperature in degrees Celsius holding black carbon fixed. We estimate that a 1 degrees Celsius increase in air temperature increases soil temperature by 0.06 (±.03, p=.07) degrees Celsius. Therefore we reject the null hypothesis that air temperature does not have an affect on soil temperature (at a significance level of 0.05).\nHypothesis 2\nOur second null hypothesis is that there is no effect of black carbon in ug/m3 on soil temperature in degrees Celsius. The alternative hypothesis being that there is an effect of black carbon in ug/m3 on soil temperature in degrees Celsius. We find a statistically significant effect whereby increases in black carbon ug/m3 increase soil temperature in degrees Celsius holding air temperature in degrees Celsius fixed. We estimate that a 1 percent increase in black carbon in ug/m3 will decrease soil temperature by 0.0036 (±.17, p=.03) degrees Celsius. Therefore we reject the null hypothesis that black carbon does not have an affect on soil temperature (at a significance level of 0.01).\nHypothesis 3\nOur third hypothesis is that the effect of black carbon on soil temperature varies across temperatures, since warmer temperatures could coincide with higher wildfire incidences. However, we do not find a statistically significant effect whereby the impact of air temperature differs for levels of black carbon present on soil temperature. The interaction term is very slightly positive but overall insignificant within our model.\nThe adjusted R2 values also increased slightly (from 0.173 to 0.179) when adding the interaction model. This demonstrates that the interaction model just slightly increases model fit.\nHowever, this adjusted R2 value also indicates that only about 17% of the variability in soil temperature is explained by the model. With this information, we can hypothesize that there are other factors that affect soil temperature not included within this model."
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#time-series",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#time-series",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "Time Series",
    "text": "Time Series\nAn additional consideration to take into account is that we know temperature experiences seasonality, but does black carbon also fluctuate throughout the year? To investigate this component we will be exploring black carbon levels across time.\n\n\nCode\n# turn permafrost df into tsibble for decompostion\npermafrost &lt;- as_tsibble(full) %&gt;% \n  fill_gaps()\n\n# filter dataset to 2019-2021\npermafrost_2020 &lt;- permafrost %&gt;% \n  filter(between(date, as.Date('2019-12-29'), as.Date('2021-01-03')))\n\n# graph the time series \npermafrost_2 &lt;- permafrost_2020 %&gt;% \n  mutate(date = date(date)) %&gt;%\n  ggplot(., aes(x = date, y = log_b_c)) +\n  geom_line() +\n  labs(x = 'Date', y = 'log(black carbon)', title = 'Time Series of Black Carbon Levels from 2019 to 2021') +\n  theme_linedraw()\n\npermafrost_2\n\n\n\n\n\nIn viewing this time series of the data from December of 2019 to January of 2021 we are able to get a continuous glimpse of black carbon levels over time. It does appear that there could be an outlier occurring in September of 2020, but overall there is not a large indicator of trend or seasonality. Upon further research September 2020 had multiple large fires burning in the Pacific Northwest whose smoke and associated particulate matter reached Alaska (Mass et al. 2021) . If we examine the full timeline of data present:\n\n\nCode\npermafrost_full &lt;- permafrost %&gt;% \n  filter(between(date, as.Date('2019-12-29'), as.Date('2023-11-19')))\n\npermafrost_3 &lt;- permafrost_full %&gt;% \n  mutate(date = date(date)) %&gt;%\n  ggplot(., aes(x = date, y = log_b_c)) +\n  geom_line() +\n  labs(x = 'Date', y = 'log(black carbon)', title = 'Time Series of Black Carbon Levels from 2019 to 2023') +\n  theme_linedraw()\n\npermafrost_3\n\n\n\n\n\nWe can see that there does not appear to be a relationship of seasonality or trend present. Unfortunately, the air sensor did not collect data from mid-September of 2021 to March of 2022. This hole in the data is not marked in the documentation of the data, but could have resulted from a sensor error. Additionally we see a very high black carbon level in September of 2022. I predict that this value comes from the fact that 2022 is considered one of Alaska’s biggest fire seasons since 1950 with over three million acres burned (Rosen 2022). This gap prevents a decomposition from being run on the entire timescale of data we have that would help determine if seasonality were an important component. However, we can examine a decomposition run from 2019 to 2020 to help determine if there is a trend present in the black carbon levels. Due to the appearance of a few outliers in our data notably September of 2020 a Seasonal and Trend Decomposition using Loess was chosen for its robustness to outliers.\n\n\nCode\npermafrost_decomp &lt;- permafrost_2020 %&gt;% \n  model(STL(log_b_c, type = \"additive\")) %&gt;% \n  components() %&gt;% \n  autoplot() +\n  labs(title = \"STL decomposition of black carbon levels at Toolik Field Station, AK\")\n\npermafrost_decomp\n\n\n\n\n\nFrom our decomposition model we can see that for black carbon levels there is no definitive trend and that the remainder component is more important than the trend for black carbon levels. As the amount of data recorded from the purple air sensor increases running this test again could be beneficial to determine if seasonality plays a role."
  },
  {
    "objectID": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#takeaways",
    "href": "blog-posts/2023-12-10-permafrost-black-carbon/index.html#takeaways",
    "title": "Soil Temperature Analysis at Toolik Field Station, AK",
    "section": "Takeaways",
    "text": "Takeaways\nWith this initial analysis we can conclude that air temperature and black carbon do have an effect on soil temperature. However, we can also see that their are factors that we have not addressed in this model to account for more of the variation in soil temperature.\nAdditional research and analysis should be done surrounding this topic. This analysis is spatially limited to Toolik Field Station in Alaska and its immediate surrounding area. This analysis could be extended to a larger geographic area of permafrost coverage. Additionally, this analysis would benefit from additional and extended data. Toolik Field Station has data on air temperature and soil temperature since 1988. However, the purple sensor which is able to record the black carbon levels, was installed in 2019. Thus, as black carbon is the main interest for my question I think it would benefit from additional time. This could also allow for a next step to be a decomposition model that is able to address seasonality.\nTo see the full repository, check out the project on Github"
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html",
    "href": "blog-posts/2023-12-01-houston-outages/index.html",
    "title": "Houston, Texas Power Outages",
    "section": "",
    "text": "February 2021, the state of Texas suffered a major energy crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20(POWER Committee, n.d.). The energy crisis that occurred resulted in a loss of power for millions of Texans, fifty-seven deaths, and billions in property damages(POWER Committee, n.d.).\nIn the following exploration adapted from Dr. Ruth Oliver we will be:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities susceptibility to power outages\nAnalysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will detect differences in night lights before and after the storm to identify areas that lost electric power."
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html#background",
    "href": "blog-posts/2023-12-01-houston-outages/index.html#background",
    "title": "Houston, Texas Power Outages",
    "section": "",
    "text": "February 2021, the state of Texas suffered a major energy crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20(POWER Committee, n.d.). The energy crisis that occurred resulted in a loss of power for millions of Texans, fifty-seven deaths, and billions in property damages(POWER Committee, n.d.).\nIn the following exploration adapted from Dr. Ruth Oliver we will be:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities susceptibility to power outages\nAnalysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will detect differences in night lights before and after the storm to identify areas that lost electric power."
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html#lets-start-coding",
    "href": "blog-posts/2023-12-01-houston-outages/index.html#lets-start-coding",
    "title": "Houston, Texas Power Outages",
    "section": "Let’s Start Coding",
    "text": "Let’s Start Coding\nThis section illustrates the code utilized to explore the effects of the storms.\n\n\nCode\n# import necessary libraries to perform analysis\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)\nlibrary(raster)\nlibrary(ggspatial)\nlibrary(cowplot)\n\n\nLoading and Exploring our Data\nTo investigate the effects of the Houston, TX storms we will use raster data primarily. The following code block uses SQL and the stars package to load in the data needed for the project.\nFor our raster data we used NASA’s Worldview data for February 7, 2021 (before the power outage) and February 16, 2021 (during the power outage). These dates were selected as other days during the time frame had cloud cover that would have skewed our investigation.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Data was downloaded and prepped in advance and stored in the .gitignore.\n\n\nCode\n# read in night lights tiles\nnight1 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif')\nnight2 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif')\nnight3 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif')\nnight4 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif')\n\n# combine tiles into stars object for 2021-02-07 and 2021-02-16\nnight_07 &lt;- st_mosaic(night1, night2)\nnight_16 &lt;- st_mosaic(night3, night4)\n\n# view 2021-02-07 raster\nplot(night_07, main = \"Raster of 2021-02-07\")\n\n\n\n\n\nNext we will minimize light emanating from major highways systems, we used geographic data from OpenStreetMap (OSM) from Geofabrik’s download sites. These data were downloaded and prepped in advance to contain a subset of highway and roads that intersect with the Houston metropolitan area. Data from Geofabrik’s download sites for residences in the Houston metropolitan area were also accessed.\n\n\nCode\n# creates the query to select motorways\nhighway_query &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# load highway data that fulfills query \nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = highway_query)\n\n# reproject highwyas to crs 3083\nhighways_reproj &lt;- st_transform(highways, crs = 3083)\n\n# creates the query to select residential buildings\nresidential_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# load buildings data that fulfills query \nresidential &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = residential_query)\n\n# reproject buildings to crs 3083\nresidential_reproj &lt;- st_transform(residential, crs = 3083)\n\n\nFor the final data set utilized, data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 from an ArcGIS file geodatabase was accessed. The metadata for each layer is available at ACS metadata.\n\n\nCode\n# load data that contains census tract geometries\ntexas_census &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer =  \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n# reproject census data to crs 3083\ntexas_census_reproj &lt;- st_transform(texas_census, crs = 3083)\n\n# load data that contains census tract data and income data\ntexas_income &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer =  \"X19_INCOME\")\n\n\nBlackout Mask\nTo find the change in night lights intensity (presumably) caused by the storm we will create a blackout mask. In order to do this we will be reclassifying the difference between the two rasters. We will be classifying any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout, and any location less than this will be classified to a NA\n\n\n\nCode\n# find the change in night light intensity between the two dates \nlight_difference &lt;- night_07-night_16\n\n# reclassifying raster to show blackouts above 200, and label areas that experienced an outage outage, and that didn't NA\nlight_difference_outage &lt;- cut(light_difference, c(200, Inf), labels = 'outage')\n\n\nThe blackout mask will then be vectorized and fixed for any invalid geometries.\n\n\n\nCode\n# vectorize the blackout mask to make this a spatial feature\nblackouts &lt;- st_as_sf(light_difference_outage) %&gt;% \n  st_make_valid()\n\n\nThe vectorized mask was then cropped to our region of interest of Houston, TX with the following coordinates being utilized: (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29). The blackout mask was then cropped to the size of Houston , TX and ultimately reprojected to EPSG:3083. The EPSG:3083 is also known as NAD83 / Texas Centric Albers Equal Area.\n\n\n\nCode\n# defining houston coordinates\nhouston &lt;- cbind(x = c(-96.5, -96.5, -94.5, -94.5, -96.5), \n                         y = c(29, 30.5, 30.5, 29, 29))\n\n#turn houston coordinates into polygon\nhouston &lt;- st_sfc(st_polygon(list(houston)), crs = 4326)\n\n# create a mask for the houston area \nhouston_mask &lt;- st_intersects(blackouts, houston, sparse = FALSE)\n\n# crop the blackout_mask to the houston region of interest \nhouston_blackouts &lt;- blackouts[houston_mask,]\n\n# reprojecting houston blackouts to Texas specific Datum\nhouston_blackouts_reproj &lt;- st_transform(houston_blackouts, crs = 3083)\n\n\nHighways and Residences Selection\nWe then excluded highways, and created a 200 meter buffer to include buildings outside of 200 meters from a highway. Additionally we filtered to the residential buildings only within the blackout mask. These processes allow us to identify the number of homes impacted by the blackouts.\n\n\n\nCode\n# identify areas within 200m of all highways\nhighways_buffer &lt;- st_buffer(highways_reproj, dist = 200) %&gt;% \n  st_union()\n\n# find areas that had blackouts that are further than 200m \nblackouts_outside_highway &lt;- st_difference(houston_blackouts_reproj, highways_buffer)\n\n# dataframe where residential buildings are kept that are in houston outside of our highway buffer\nblackout_residential &lt;- residential_reproj[blackouts_outside_highway, , op = st_intersects]\n\n#counting the number of residential buildings that got hit\nprint(paste0(nrow(blackout_residential), ' residential buildings were affected by the blackouts'))\n\n\n[1] \"157411 residential buildings were affected by the blackouts\"\n\n\nSocioeconomic Effects\nTo explore if blackouts were correlated with socioeconomic factors like median income, we used census tract data. We used spatial joins to join the census tract data, and buildings that were and were not impacted by blackouts.\n\n\n\nCode\n# create data frame that contains median income data and geoID\ntexas_median_income &lt;- texas_income %&gt;% \n  dplyr::select(B19013e1, GEOID) %&gt;% \n  rename(median_income = B19013e1, GEOID_Data = GEOID)\n\n# join the income data to census tract geometries in Texas\ntexas_income_census &lt;- left_join(texas_census_reproj, texas_median_income, by = \"GEOID_Data\")\n\n# join the census tract data with residential buildings determined to be impacted by blackouts\ntexas_census_blackouts &lt;- st_filter(texas_income_census, blackout_residential) \n\n\nComparison of median income for census tracts impacted by blackouts and those that were not\nTo compare incomes of impacted census tracts and unimpacted census tracts a map of median income per census tract with the tracts that had blackouts outlined was created.\n\n\nCode\n#transforming houston boundary to Texas datum\nhouston_reproj &lt;- st_transform(houston, crs = 3083)\n\n# data frame that has all median income is in houston census tracts\nhouston_income_census &lt;- st_filter(texas_income_census, houston_reproj)\n\n# create visualization\nmedian_income_map &lt;- ggplot() +\n  geom_sf(data = houston_income_census, # plot the median income of all of houston\n          aes(fill = median_income)) +\n  scale_fill_viridis_c() +\n  geom_sf(data = texas_census_blackouts, aes(color = \"red\", # outline census tracts that experienced blackouts in red\n                                       fill = median_income)) +\n  scale_color_hue(labels = c(\"Blackout Census Tracts\")) + # alter legend titles\n  labs(fill='Median Income') +\n  theme_void() +\n  annotation_north_arrow( # add north arrow\n  height = unit(1.5, \"cm\"),\n  width = unit(1.5, \"cm\"),\n  pad_x = unit(7.5, \"cm\"),\n  pad_y = unit(0.15, \"cm\"),\n  rotation = NULL,\n  style = north_arrow_fancy_orienteering\n) +\n  annotation_scale( # add scale bar\n  plot_unit = NULL,\n  bar_cols = c(\"black\", \"white\"),\n  line_width = 1,\n  height = unit(0.15, \"cm\"),\n  pad_x = unit(2, \"cm\"),\n  pad_y = unit(0.15, \"cm\"),\n  text_pad = unit(0.15, \"cm\"),\n  text_cex = 0.7,\n  text_face = NULL,\n  text_family = \"\",\n  tick_height = 0.6\n) +\n  labs(title= \"Median Income of Census Tracts in Houston, TX\", color = \"\") # add map title\n  \nmedian_income_map\n\n\n\n\n\nTo further explore the impacts of blackouts on census tracts the distribution of income in impacted and unimpacted tracts was plotted.\n\n\n\nCode\n# find areas that did not have blackouts within the houston area \nunimpacted &lt;- houston_income_census[texas_census_blackouts, , op = st_difference]\n\n# create a histogram showing median income spread for homes in houston not impacted by blackouts \nunimpacted_figure &lt;- ggplot(data = unimpacted, aes(median_income)) +\n  geom_histogram(bins=50, fill = '#481467FF', color = 'white') +\n  labs(title = \"Unimpacted by Blackouts\", x = \"Median Income ($)\", y = \"Census Tracts\") +\n  theme_classic() +\n  ylim(0,105)\n  \n#unimpacted_figure\n\n# create a histogram showing median income spread for homes in houston impacted by blackouts\nimpacted_figure &lt;- ggplot(data = texas_census_blackouts, aes(median_income)) +\n  geom_histogram(bins=50, fill = '#481467FF', color = 'white') +\n  labs(title = \"Impacted by Blackouts\", x = \"Median Income ($)\", y = \"Census Tracts\") +\n  theme_classic() +\n  ylim(0,105)\n\n# position histograms side by side for easier comparison\nimpact_unimpact_plot &lt;- plot_grid(unimpacted_figure, impacted_figure, labels = c('', ''), label_size = 12)\n\nimpact_unimpact_plot"
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html#takeaways",
    "href": "blog-posts/2023-12-01-houston-outages/index.html#takeaways",
    "title": "Houston, Texas Power Outages",
    "section": "Takeaways",
    "text": "Takeaways\nThe median income of homes in Houston, TX affected and not affected by blackouts does not appear to have a significant difference. The blackouts do appear to have centered around the center of the city, but this area also has a wide variety of median incomes. Additionally, when comparing the median incomes of homes affected and not affected the distributions appear to be similar for the spread of wealth. Residents not impacted by the storm do appear to have slightly higher median incomes which could coincide with living farther outside the city center in the suburbs of Houston. A limitation of this analysis is that we did reduce the number of buildings counted that fell close to highways, which could be areas that have lower median incomes. Additionally, in exploring the power outages in Houston, TX we only compared the blackouts from the first two storms, without accounting for the third storm. Without accounting for the third storm the total number of outages that occurred is being underestimated.\nThis goal of this exploration was to become more familiar with spatial data. The results and findings of this investigation are not conclusive and should not be utilized without additional investigations. Overall, I hope this blog post was helpful in learning how different packages and functions can be used for working with spatial data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Melissa Widas",
    "section": "",
    "text": "Hello! My name is Melissa Widas (my last name is pronounced We-dis). I am an environmental data scientist who loves creating engaging and accessible data visualizations and learning more about the world around me. When I am not coding away at my desk, I am experimenting with new recipes (with a relatively good success rate), and traveling to new places. Please explore around my website to learn more about me and what I am working on."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html",
    "title": "Data Visualization Accessibility",
    "section": "",
    "text": "The topic I would like to focus on for this blog post centers around data justice and data accessibility to individuals that have disabilities and how our visualizations and presentation of data matter.\nOne of the key roles that data science plays is in the effective communication of data. Data visualizations are the main vehicle that allows data scientists to effectively convey findings and reach a broader audience. A data visualization is “the representation of data through use of common graphics” (IBM, n.d.). However, in order to promote equitable access to data and its findings it is important to consider that not all people are able to process data the same way. In the United States alone it is estimated that roughly twenty-five percent of the adult population has some sort of disability (Kahn 2023). Disabilities can range from cognitive, auditory, visual, and other levels of ability. Each of these disabilities can affect how someone is able to intake data and interpret data visualizations.\n\n\n\n\n\nThe following blog post aims to explore barriers for disabled persons to data access and actions data scientists can take in order to maintain data justice through accessible data access and scientific communication.\nThe main disabilities I will be exploring in the following sections are cognitive, and visual.\nThe following data visualization made from the PalmerPenguin package will be edited as well throughout this blog post highlighting tools mentioned to increase the accessibility of this visualization.\n\n\nCode\nnoaxes &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  theme(axis.text.x=element_blank(), \n      axis.ticks.x=element_blank(), \n      axis.text.y=element_blank(), \n      axis.ticks.y=element_blank())\n\nnoaxes"
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#background",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#background",
    "title": "Data Visualization Accessibility",
    "section": "",
    "text": "The topic I would like to focus on for this blog post centers around data justice and data accessibility to individuals that have disabilities and how our visualizations and presentation of data matter.\nOne of the key roles that data science plays is in the effective communication of data. Data visualizations are the main vehicle that allows data scientists to effectively convey findings and reach a broader audience. A data visualization is “the representation of data through use of common graphics” (IBM, n.d.). However, in order to promote equitable access to data and its findings it is important to consider that not all people are able to process data the same way. In the United States alone it is estimated that roughly twenty-five percent of the adult population has some sort of disability (Kahn 2023). Disabilities can range from cognitive, auditory, visual, and other levels of ability. Each of these disabilities can affect how someone is able to intake data and interpret data visualizations.\n\n\n\n\n\nThe following blog post aims to explore barriers for disabled persons to data access and actions data scientists can take in order to maintain data justice through accessible data access and scientific communication.\nThe main disabilities I will be exploring in the following sections are cognitive, and visual.\nThe following data visualization made from the PalmerPenguin package will be edited as well throughout this blog post highlighting tools mentioned to increase the accessibility of this visualization.\n\n\nCode\nnoaxes &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  theme(axis.text.x=element_blank(), \n      axis.ticks.x=element_blank(), \n      axis.text.y=element_blank(), \n      axis.ticks.y=element_blank())\n\nnoaxes"
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#cognitive",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#cognitive",
    "title": "Data Visualization Accessibility",
    "section": "Cognitive",
    "text": "Cognitive\nCognitive disability, which can also be commonly referred to as Intellectual and Developmental Disabilities (IDD), has been a group of individuals often absent from the conversation around data accessibility. The following considerations and recommendation are from the research that has been conducted in the data visualization space for an audience that has IDD. Historically individuals with IDD have been pushed to the side of conversations regarding their own data as well as data around them that they would like to interact with. The main component of data visualization that data scientists consider when creating visualizations for those with cognitive disabilities or to be accessible to those with cognitive disabilities are how the data are presented. For cognitive disabilities when creating visualizations there are some specific considerations to keep in mind.\n\nData Design\nRethinking how we can present data to those with IDD is at the forefront of creating data that is more accessible for those with IDD. An emerging form of data science presentation is through the use of touch. Data scientists and artists are pairing up in order to create interactive displays and sculptures that present data. These data can be shown through the use of shapes, colors, and scaled sizing in an approachable format that prioritizes a major theme for each sculpture or installation(Corona 2019). Data presented in this way can allow individuals who might be unable to comprehend a visualization with many individual pieces the opportunity to interact with the data and understand the takeaway being shown.\nAn additional consideration when designing a data visualization is the type of figure that will be used. Research done at the University of Chapel Hill has revealed that pie charts are not accessible to those with IDD(Wu and Albers Szafir 2023). Thus, researchers and data scientists should consider if there are other visualizations that can be utilized.\nFurthermore, something that data scientists should always consider is how to emphasize the main takeaway from their visualization. Data scientists should consider the level of visual complexity they are presenting and potentially isolate different takeaways with different figures. Clearly presenting the key finding of a figure can allow everybody to better understand a data visualization.\n\n\nData Visualization\nWhen creating a data visualization and having the potential for an audience that has cognitive disabilities there are some design components that can be added or highlighted which will in general increase comprehension.\n\nLabeling\nData scientists should ensure that labeling is intuitive, As an example, if data is being displayed regarding money, an easy to add label would be the $(Wu and Albers Szafir 2023).\n\n\nAxes\nIt is important to ensure that data presented in visualizations has clear start and end points(Wu and Albers Szafir 2023). Indication of where data is beginning and where data is ending on a visualization such as axes marks allows an audience member to definitively know where they should stop visually processing.\n\n\nCode\naxes &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  labs(x = \"Penguin Species\", y = \"Penguin flipper length (mm)\", title = \"Penguin flipper length by Species\") +\n         geom_jitter()\n\naxes\n\n\n\n\n\n\n\nTitles\nInformative titles can also be used to improve comprehension of a visualization(Knaflic Nussbaumer 2018). A title that includes the key takeaway from a visualization allows users to look for something in the data that supports the title which can increase comprehension.\n\n\nCode\ntitle &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  labs(x = \"Penguin Species\", y = \"Penguin flipper length (mm)\", title = \"Penguin flipper length by Species\", subtitle = \"Adelie penguins show largest flipper length\") +\n         geom_jitter()\n\ntitle"
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#visual",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#visual",
    "title": "Data Visualization Accessibility",
    "section": "Visual",
    "text": "Visual\nVisual disability is a large and varied field. It is estimated that roughly eight percent of the U.S. population has blindness or a visual impairment that affects their daily life. Visual impairments is considered an umbrella term and is more concretely defined as vision that is unable to be corrected through the use of aids (glasses, or contacts) or surgery(Institute, n.d.). It is important to consider how data visualizations are created in order to maintain accessibility for users who have visual impairments.\n\nData Design\nAn emerging form of data representation is through the use of sounds. Sonified data uses non-speech sounds to explore and describe data. Data scientists can import data and create graphics through programs like the SAS Graphics Accelerator. The SAS Graphics accelerator enables scientists and the audiences they are trying to reach to access these data independently and create a framework they can succeed in.\n\nAlternative Text\nData scientists can add alternative text (commonly referred to as Alt Text) to data visualizations. Alternative texts are often labeled with figure titles which provide very little information to someone who cannot clearly see the visualization. Alternative text should instead be thought of as a typed explanation of the figure. By providing more details in the alternative text adaptive technologies, like screen readers, can read the figure details to the user. Important things to include in the alt text would be the title, key trends, and a potential link to data in a readable format like a csv file(Nussbaumer Knaflic 2018). An additional consideration is that screen readers do not allow the user to alter or skip what is being read so it is important to ensure that the text you include is well thought out but a reasonable length.\n\n\nAuditory Explanations\nFigures and visualizations can also include a link or a sound-bite that walks a user through the figure. The use of auditory explanations allows users to see and explore data through the creators eyes as well as potentially receive a more-detailed explanation than the alternative text.\n\n\n\nData Visualization\n\nColor\nA potentially more common visual impairment is colorblindness. There are packages within R and Python that are specifically designed and built for people with color blindness(Ou 2021). Using colorblind-friendly color schemes is good practice to increase the accessibility of any chart or figure that is using color to differentiate data. Some examples of color-blind friendly palettes are the following:\n\nmunsell\nviridis and RColorBrewer\ndichromat\ncolorblindr\nshades\nggsci\n\n\n\nCode\nfriendly &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  labs(x = \"Penguin Species\", y = \"Penguin flipper length (mm)\", title = \"Penguin flipper length by Species\") +\n         geom_jitter() +\n  scale_color_viridis(discrete=TRUE, begin =0, end = 0.75)\n\nfriendly\n\n\n\n\n\n\n\nHigh contrast elements\nHigh contrast elements also allow for increased readability for those with visual impairments(nussbaumerknaflic2018?). The contrast between colors of elements and the sizes of these elements can affect the visual ease of use for users. There are multiple tools that data scientists can utilize to check the contrast of their images and even design a palette they would like to use.\n\n\nLabeling Data\nA barrier to users with low vision can be legends(nussbaumerknaflic2018?). Legends can typically include smaller text that is positioned outside of where the bulk of data is shown through the figure. This can create a disconnect between the data shown and any qualifiers assigned to it like scale, or type. A possible solution for this is direct labeling. Direct labeling can be labels next to data or within the data visualization. A package that can be used is the directlabels package.\n\n\nCode\nlabel &lt;- ggplot(data = penguins, aes(x = bill_length_mm,\n    y = bill_depth_mm, color = species)) +\n         geom_point() +\n  labs(x = \"Penguin Species\", y = \"Penguin flipper length (mm)\", title = \"Penguin flipper length by Species\") +\n         geom_jitter() +\n  scale_color_viridis(discrete=TRUE, begin=0, end = 0.75) +\n  theme(legend.position=\"none\") \n  \nlabel2 &lt;- direct.label(label, method = NULL, \n    debug = FALSE) \n  \nlabel2"
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#environmental-data",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#environmental-data",
    "title": "Data Visualization Accessibility",
    "section": "Environmental Data",
    "text": "Environmental Data\nAs the field of data science continues to grow and be at the forefront of scientific communication, especially environmental scientific communication, we as data scientists need to keep in mind how our visualizations and findings are received. Environmental analyses, and visualizations are some of the most impactful tools scientists have to educate the public on environmental challenges. In order to create the most cutting edge solutions that prioritize people and the environment around us, we as data scientists need to ensure people of all backgrounds are at the table making decisions. Everyone, regardless of their ability, have the right to access data and what we can learn from it."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#takeaway",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#takeaway",
    "title": "Data Visualization Accessibility",
    "section": "Takeaway",
    "text": "Takeaway\nWe have covered numerous strategies in order to make data visualizations more accessible. Now depending on the goal of a researcher doing every one of these strategies could be an impossible task considering timelines and budgets. However, it is important for data scientists to think critically about who their audience is and if some of the strategies discussed can be applied. Some additions and considerations like, incorporating alternative text that is clear and concise and high contrast color palettes take minimal time and are often pre-built into some packages. Data scientists of all backgrounds, including within the environmental realm have the responsibility to consider how their data visualizations are able to be received and by what audiences."
  },
  {
    "objectID": "blog-posts/2023-12-06-ca-land-class/landcover-statistics-ca.html",
    "href": "blog-posts/2023-12-06-ca-land-class/landcover-statistics-ca.html",
    "title": "Mount Whitney, California Land Cover",
    "section": "",
    "text": "Mount Whitney, California Land Cover Types\nGithub Repository\n\nBackground\nIn this notebook an exploration of land cover types around Mount Whitney, California is conducted. A bar graph showing percentages of land cover and a visual representation of where Mount Whitney is located within California will be the deliverables created.\n\n\nHighlights\n\nData wrangling and exploration with pandas\nGeospatial data wrangling with geopandas and rioxarray\nMerging of tabular and vector data\nCreating and customizing a map\nCreating and customizing a horizontal bar plot with matplotlib\n\n\n\nData\nFirst dataset\nA small section of the GAP/LANDFIRE National Terrestrial Ecosystems data for 2011, from the US Geological Survey (USGS). This is a raster file with a 30 m x 30 m pixel resolution. Each cell in the raster has a number representing the type of land cover.\nThe data was pre-processed in the Microsoft Planetary Computer to show a small region around Mount Whitney, California.\nFurther information about the dataset can be accessed via the the dataset’s Digital Object Identifier (DOI) link.\nSecond dataset\nA shapefile of CA Geographic Boundaries. This is a subset of the US Census Bureau’s 2016 TIGER database, which only has the state boundary. The URL for this data can be accessed here.\nThird dataset\nA csv file with the name of the land cover associated with each code.\n\n\nFinal Visualizations\nThe final visualizations produced in this exploration will be a map for geographic context to indicate where our data around Mount Whitney is located from, and a figure highlighting the dominant land cover/land use types around Mount Whitney.\n\n\n\nmap\n\n\n\n\n\nbar plot showing dominant land class types\n\n\n\n\nLet’s Get Coding\n\n\nLoad necessary packages\n\n\nCode\n# import libraries and functions needed\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\nimport matplotlib.lines as mlines # for creating legend shapes\n\nimport xarray as xr\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import Point\nfrom shapely.geometry import box\n\n\n\n\n\nImport Land Cover data\nI downloaded the .tif file containing land cover information for the area around Mount Whitney, CA and saved this as lulc short for land use/land cover. rioxarray and os allow for the land cover data to be opened as a raster and allows for the exploration of the file, like exploring the projection it is in.\n\n\nCode\n# load in .tif file\nlulc_fp = os.path.join(os.getcwd(),'data','land_cover.tif')\n\n# store .tif file as lulc\nlulc = rioxr.open_rasterio(lulc_fp)\n\n# plot the lulc raster\nlulc.plot()\n\n\n&lt;matplotlib.collections.QuadMesh at 0x188e6e9d0&gt;\n\n\n\n\n\n\n\nImport outline of California\nI used a URL that links to the California shapefile in order to access the shape of California.\n\n\nCode\n# load ca shapefile\nca = gpd.read_file(\"https://data.ca.gov/dataset/e212e397-1277-4df3-8c22-40721b095f33/resource/3db1e426-fb51-44f5-82d5-a54d7c6e188b/download/ca-state-boundary.zip\")\n\n# look at ca\nca.plot()\n\n# update ca to the same crs as lulc\nca = ca.to_crs('epsg:5070')\n\n\n\n\n\n\n\nCreating a Bounding Box and Point for Mt. Whitney\nTo create the map showing where the lulc tile is within the state of California and how that related to Mount Whitney we will be creating a bounding box based off of lulc and a point created from coordinates from Mount Whitney.\n\n\nCode\n# create a GeoDataFrame with the lulc bounding box\nbbox = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*lulc.rio.bounds())]})\n\n# create geodataframe with mount whitney point\nmt_whitney = gpd.GeoDataFrame(geometry=[Point(-118.2923, 36.5785)],\n                           crs='epsg:4326')\n\n\n# update mt_whitneys crs to the same crs as lulc\nmt_whitney = mt_whitney.to_crs('epsg:5070')\n\n\n\n\nCreating a Map to Provide Geographic Context\nNow that we have finished loading and updating our data we can explore the spatial relationship of our data. We will use matplotlib to create a map that highlights the location of Mount Whitney in relation to the area of land use/land cover data we have within the state of California.\n\n\nCode\n# create figure\nfig, ax = plt.subplots(figsize=(12,8))\n\nax.axis('off')\n\nca.plot(ax=ax,  # add california outline\n       color='#fef0c2',\n       edgecolor='black')\nca_patch = mpatches.Patch(color='#fef0c2',\n                          label='California, US')\n\nbbox.plot(ax=ax,   # add lulc bounding box\n         color ='green',\n         edgecolor ='black',\n         label ='LULC tile')\nbbox_patch = mpatches.Patch(color='green',\n                            label='LULC tile')\n\nmt_whitney.plot(ax=ax,     # add marker for Mount Whitney\n               color = 'red',\n               marker = 6)\n\nmt_whitney_triangle = mlines.Line2D([], [], color='red',\n                                    marker=6,\n                                    markersize=10,\n                                    label='Mount Whitney',\n                                   lw=0)\n\nax.legend(handles = [ca_patch, bbox_patch, mt_whitney_triangle], frameon=False, loc=(0.75, 0.75))\n\n# --------------------------------------\n# save figure\nplt.savefig('images/ca_mt_whitney_map.png', bbox_inches='tight',  dpi=100)\n\nplt.show()\n\n\n\n\n\n\n\nImport Land Cover Type Data\nI downloaded data that contained land cover type and accompanying codes in order to explore the land cover types present in the lulc raster.\n\n\nCode\n# import the tabular data \nclass_names = pd.read_csv('data/GAP_National_Terrestrial_Ecosystems.csv')\n\nclass_names.head()\n\n\n\n\n\n\n\n\n\nclass_label\ncode\n\n\n\n\n0\n0\n0\n\n\n1\nSouth Florida Bayhead Swamp\n1\n\n\n2\nSouth Florida Cypress Dome\n2\n\n\n3\nSouth Florida Dwarf Cypress Savanna\n3\n\n\n4\nSouth Florida Mangrove Swamp\n4\n\n\n\n\n\n\n\n\n\nLulc Raster Exploration\nIn order to more effectively use our lulc raster the squeeze() and drop() functions will be applied.\n\n\nRaster Reduction\n\n\nCode\n# remove length 1 dimension (band)\nlulc = lulc.squeeze()\n\n# remove coordinates associated to band\nlulc = lulc.drop('band')\n\n\n\n\nCalculation of Area Covered by Land Cover Class\nCreate a data frame that will store the percentages covered by each land cover class.\n\n\nCode\n# get the number of pixels per class in lulc\npixels = np.unique(lulc, return_counts=True)\n\n# initialize dictionary with pixels arrays data \npix = {'code' : pixels[0],\n     'number_pixels' : pixels[1],\n     }\n\n# create data frame\npix_counts = pd.DataFrame(pix)\n\n# add class names\nclasses = pd.merge(pix_counts,\n                   class_names,\n                   how = 'left',\n                   on = 'code')\n\n# calculate the total_pixels from attributes of lulc\ntotal_pixels = classes['number_pixels'].sum()\n\n# add the percentage of area covered by each class and round to 8 decimal points\nclasses['percentage_covered'] = round((classes['number_pixels']/total_pixels*100), 8)\n\n\n\n\nVisualization of Land Cover Types\nNow that we have calculated how much of our raster is covered by each land cover, let’s create a bar plot to help us visualize the dominant types. Using matplotlib a horizontal bar plot is made that highlights the classes with more than 1% land cover in decreasing order.\n\n\nCode\n# create horixontal bar plot showing the classes with more than 1% land cover in decreasing order\nclasses.loc[(classes['percentage_covered']&gt;1)].sort_values(by='percentage_covered', ascending=True).plot.barh(x='class_label',\n                                                        y='percentage_covered',\n                                                                                                              xlabel = \"Percentage Land Cover\",\n                                                                                                              ylabel = \"Land Cover Class\",\n                                                                                                              color = \"green\",\n                                                                                                              legend = False)\n                                                                                                              \n\n# -----------------------------\n# save figure\nplt.savefig('images/percent_land_cover_plot.png', bbox_inches='tight',  dpi=100)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{widas2023,\n  author = {Widas, Melissa},\n  title = {Mount {Whitney,} {California} {Land} {Cover}},\n  date = {2023-12-06},\n  url = {http://mwidas.github.io/2023-12-06-ca-land-class},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nWidas, Melissa. 2023. “Mount Whitney, California Land\nCover.” December 6, 2023. http://mwidas.github.io/2023-12-06-ca-land-class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Melissa Widas earned a Bachelor’s of science degree from Montana State University in Environmental Sciences with a focus in land rehabilitation in 2022. During her time at Montana State, Melissa found a passion for data science through a research internship with the GeoSWIRL Lab. This opportunity resulted in the creation and management of a drought visualization tool for watersheds across the state of Montana. The development of this tool relied on the use of geographic information sciences and performed statistical analyses to highlight watershed change and risk of drought within watersheds. After experiencing a taste of data science, Melissa wanted to expand her knowledge to explore its effect on environmental practices. She is now pursuing a Master’s of Environmental Data Science at the Bren School at the University of California at Santa Barbara. Melissa is looking forward to using the power of data science and data visualization to help answer environmental questions regarding habitat restoration, remediation, and the efficacy of these practices to ameliorate the lands around her.\n\n\n\n\n\n\n\n\n\n\nI had the extraordinary opportunity to visit the Anaconda smelter in Anaconda, MT that was a vital part of the copper boom in Montana, but also contributed to a legacy of mining waste and remediation efforts in Anaconda."
  },
  {
    "objectID": "about.html#fa-laptop-code-titlelaptop-code-professional-interests",
    "href": "about.html#fa-laptop-code-titlelaptop-code-professional-interests",
    "title": "About",
    "section": "",
    "text": "Melissa Widas earned a Bachelor’s of science degree from Montana State University in Environmental Sciences with a focus in land rehabilitation in 2022. During her time at Montana State, Melissa found a passion for data science through a research internship with the GeoSWIRL Lab. This opportunity resulted in the creation and management of a drought visualization tool for watersheds across the state of Montana. The development of this tool relied on the use of geographic information sciences and performed statistical analyses to highlight watershed change and risk of drought within watersheds. After experiencing a taste of data science, Melissa wanted to expand her knowledge to explore its effect on environmental practices. She is now pursuing a Master’s of Environmental Data Science at the Bren School at the University of California at Santa Barbara. Melissa is looking forward to using the power of data science and data visualization to help answer environmental questions regarding habitat restoration, remediation, and the efficacy of these practices to ameliorate the lands around her.\n\n\n\n\n\n\n\n\n\n\nI had the extraordinary opportunity to visit the Anaconda smelter in Anaconda, MT that was a vital part of the copper boom in Montana, but also contributed to a legacy of mining waste and remediation efforts in Anaconda."
  }
]