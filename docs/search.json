[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Mount Whitney, California Land Cover\n\n\n\nMEDS\n\n\nGeospatial\n\n\nPython\n\n\n\nExploring land cover types and their dominance on the landscape around Mount Whitney, California\n\n\n\nMelissa Widas\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouston, Texas Power Outages\n\n\n\nMEDS\n\n\nGeospatial\n\n\nR\n\n\n\nIdentifying homes affected by severe winter storms in Houston, Texas energy crisis\n\n\n\nMelissa Widas\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Accessibility\n\n\n\nQuarto\n\n\nMEDS\n\n\nEthics\n\n\n\nExamining ways data scientists can create data that is more widely accessable\n\n\n\nMelissa Widas\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-posts/2023-12-06-ca-land-class/landcover-statistics-ca.html",
    "href": "blog-posts/2023-12-06-ca-land-class/landcover-statistics-ca.html",
    "title": "Mount Whitney, California Land Cover",
    "section": "",
    "text": "Mount Whitney, California Land Cover Types\nGithub Repository\n\nBackground\nIn this notebook an exploration of land cover types around Mount Whitney, California is conducted. A bar graph showing percentages of land cover and a visual representation of where Mount Whitney is located within California will be the deliverables created.\n\n\nHighlights\n\nData wrangling and exploration with pandas\nGeospatial data wrangling with geopandas and rioxarray\nMerging of tabular and vector data\nCreating and customizing a map\nCreating and customizing a horizontal bar plot with matplotlib\n\n\n\nData\nFirst dataset\nA small section of the GAP/LANDFIRE National Terrestrial Ecosystems data for 2011, from the US Geological Survey (USGS). This is a raster file with a 30 m x 30 m pixel resolution. Each cell in the raster has a number representing the type of land cover.\nThe data was pre-processed in the Microsoft Planetary Computer to show a small region around Mount Whitney, California.\nFurther information about the dataset can be accessed via the the dataset’s Digital Object Identifier (DOI) link.\nSecond dataset\nA shapefile of CA Geographic Boundaries. This is a subset of the US Census Bureau’s 2016 TIGER database, which only has the state boundary. The URL for this data can be accessed here.\nThird dataset\nA csv file with the name of the land cover associated with each code.\n\n\nFinal Visualizations\nThe final visualizations produced in this exploration will be a map for geographic context to indicate where our data around Mount Whitney is located from, and a figure highlighting the dominant land cover/land use types around Mount Whitney.\n\n\n\nmap\n\n\n\n\n\nbar plot showing dominant land class types\n\n\n\n\nLet’s Get Coding\n\n\nLoad necessary packages\n\n\nCode\n# import libraries and functions needed\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\nimport matplotlib.lines as mlines # for creating legend shapes\n\nimport xarray as xr\nimport rioxarray as rioxr\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import Point\nfrom shapely.geometry import box\n\n\n\n\n\nImport Land Cover data\nI downloaded the .tif file containing land cover information for the area around Mount Whitney, CA and saved this as lulc short for land use/land cover. rioxarray and os allow for the land cover data to be opened as a raster and allows for the exploration of the file, like exploring the projection it is in.\n\n\nCode\n# load in .tif file\nlulc_fp = os.path.join(os.getcwd(),'data','land_cover.tif')\n\n# store .tif file as lulc\nlulc = rioxr.open_rasterio(lulc_fp)\n\n# plot the lulc raster\nlulc.plot()\n\n\n&lt;matplotlib.collections.QuadMesh at 0x18a571250&gt;\n\n\n\n\n\n\n\nImport outline of California\nI used a URL that links to the California shapefile in order to access the shape of California.\n\n\nCode\n# load ca shapefile\nca = gpd.read_file(\"https://data.ca.gov/dataset/e212e397-1277-4df3-8c22-40721b095f33/resource/3db1e426-fb51-44f5-82d5-a54d7c6e188b/download/ca-state-boundary.zip\")\n\n# look at ca\nca.plot()\n\n# update ca to the same crs as lulc\nca = ca.to_crs('epsg:5070')\n\n\n\n\n\n\n\nCreating a Bounding Box and Point for Mt. Whitney\nTo create the map showing where the lulc tile is within the state of California and how that related to Mount Whitney we will be creating a bounding box based off of lulc and a point created from coordinates from Mount Whitney.\n\n\nCode\n# create a GeoDataFrame with the lulc bounding box\nbbox = gpd.GeoDataFrame({\"id\":1,\"geometry\":[box(*lulc.rio.bounds())]})\n\n# create geodataframe with mount whitney point\nmt_whitney = gpd.GeoDataFrame(geometry=[Point(-118.2923, 36.5785)],\n                           crs='epsg:4326')\n\n\n# update mt_whitneys crs to the same crs as lulc\nmt_whitney = mt_whitney.to_crs('epsg:5070')\n\n\n\n\nCreating a Map to Provide Geographic Context\nNow that we have finished loading and updating our data we can explore the spatial relationship of our data. We will use matplotlib to create a map that highlights the location of Mount Whitney in relation to the area of land use/land cover data we have within the state of California.\n\n\nCode\n# create figure\nfig, ax = plt.subplots(figsize=(12,8))\n\nax.axis('off')\n\nca.plot(ax=ax,  # add california outline\n       color='#fef0c2',\n       edgecolor='black')\nca_patch = mpatches.Patch(color='#fef0c2',\n                          label='California, US')\n\nbbox.plot(ax=ax,   # add lulc bounding box\n         color ='green',\n         edgecolor ='black',\n         label ='LULC tile')\nbbox_patch = mpatches.Patch(color='green',\n                            label='LULC tile')\n\nmt_whitney.plot(ax=ax,     # add marker for Mount Whitney\n               color = 'red',\n               marker = 6)\n\nmt_whitney_triangle = mlines.Line2D([], [], color='red',\n                                    marker=6,\n                                    markersize=10,\n                                    label='Mount Whitney',\n                                   lw=0)\n\nax.legend(handles = [ca_patch, bbox_patch, mt_whitney_triangle], frameon=False, loc=(0.75, 0.75))\n\n# --------------------------------------\n# save figure\nplt.savefig('images/ca_mt_whitney_map.png', bbox_inches='tight',  dpi=100)\n\nplt.show()\n\n\n\n\n\n\n\nImport Land Cover Type Data\nI downloaded data that contained land cover type and accompanying codes in order to explore the land cover types present in the lulc raster.\n\n\nCode\n# import the tabular data \nclass_names = pd.read_csv('data/GAP_National_Terrestrial_Ecosystems.csv')\n\nclass_names.head()\n\n\n\n\n\n\n\n\n\nclass_label\ncode\n\n\n\n\n0\n0\n0\n\n\n1\nSouth Florida Bayhead Swamp\n1\n\n\n2\nSouth Florida Cypress Dome\n2\n\n\n3\nSouth Florida Dwarf Cypress Savanna\n3\n\n\n4\nSouth Florida Mangrove Swamp\n4\n\n\n\n\n\n\n\n\n\nLulc Raster Exploration\nIn order to more effectively use our lulc raster the squeeze() and drop() functions will be applied.\n\n\nRaster Reduction\n\n\nCode\n# remove length 1 dimension (band)\nlulc = lulc.squeeze()\n\n# remove coordinates associated to band\nlulc = lulc.drop('band')\n\n\n\n\nCalculation of Area Covered by Land Cover Class\nCreate a data frame that will store the percentages covered by each land cover class.\n\n\nCode\n# get the number of pixels per class in lulc\npixels = np.unique(lulc, return_counts=True)\n\n# initialize dictionary with pixels arrays data \npix = {'code' : pixels[0],\n     'number_pixels' : pixels[1],\n     }\n\n# create data frame\npix_counts = pd.DataFrame(pix)\n\n# add class names\nclasses = pd.merge(pix_counts,\n                   class_names,\n                   how = 'left',\n                   on = 'code')\n\n# calculate the total_pixels from attributes of lulc\ntotal_pixels = classes['number_pixels'].sum()\n\n# add the percentage of area covered by each class and round to 8 decimal points\nclasses['percentage_covered'] = round((classes['number_pixels']/total_pixels*100), 8)\n\n\n\n\n\n\n\n\n\ncode\nnumber_pixels\nclass_label\npercentage_covered\n\n\n\n\n0\n39\n639385\nCalifornia Central Valley Mixed Oak Savanna\n7.104278\n\n\n1\n42\n584143\nCalifornia Lower Montane Blue Oak-Foothill Pin...\n6.490478\n\n\n2\n56\n518722\nMediterranean California Mixed Oak Woodland\n5.763578\n\n\n3\n146\n157\nInter-Mountain Basins Subalpine Limber-Bristle...\n0.001744\n\n\n4\n148\n121486\nRocky Mountain Aspen Forest and Woodland\n1.349844\n\n\n\n\n\n\n\n\n\nVisualization of Land Cover Types\nNow that we have calculated how much of our raster is covered by each land cover, let’s create a bar plot to help us visualize the dominant types. Using matplotlib a horizontal bar plot is made that highlights the classes with more than 1% land cover in decreasing order.\n\n\nCode\n# create horixontal bar plot showing the classes with more than 1% land cover in decreasing order\nclasses.loc[(classes['percentage_covered']&gt;1)].sort_values(by='percentage_covered', ascending=True).plot.barh(x='class_label',\n                                                        y='percentage_covered',\n                                                                                                              xlabel = \"Percentage\",\n                                                                                                              ylabel = \"Land Cover Class\")\n\n# -----------------------------\n# save figure\nplt.savefig('images/percent_land_cover_bar_plot.png', bbox_inches='tight',  dpi=100)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{widas2023,\n  author = {Widas, Melissa},\n  title = {Mount {Whitney,} {California} {Land} {Cover}},\n  date = {2023-12-06},\n  url = {http://mwidas.github.io/2023-12-06-ca-land-class},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nWidas, Melissa. 2023. “Mount Whitney, California Land\nCover.” December 6, 2023. http://mwidas.github.io/2023-12-06-ca-land-class."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html",
    "title": "Data Accessibility",
    "section": "",
    "text": "The topic I would like to focus on for this blog post centers around data justice and data accessibility to individuals that have disabilities and how our visualizations and presentation of data matter.\nOne of the key roles that data science plays is in the effective communication of data. Data visualizations are the main vehicle that allows data scientists to effectively convey findings and reach a broader audience. A data visualization is “the representation of data through use of common graphics” (IBM, n.d.). However, in order to promote equitable access to data and its findings it is important to consider that not all people are able to process data the same way. In the United States alone it is estimated that roughly twenty-five percent of the adult population has some sort of disability (Kahn 2023). Disabilities can range from cognitive, auditory, visual, and other levels of ability. And each of these disabilities can affect how someone is able to intake data and data visualizations.\n\n\n\n\n\nThe following blog post aims to explore barriers for disabled persons to data access and actions data scientists can take in order to maintain data justice through data access and accessibility for all.\nThe main disabilities I will be exploring in the following sections are cognitive, and visual.\n\n\n\nCognitive disability, which can also be commonly referred to as Intellectual and Developmental Disabilities (IDD), has been a group of individuals often absent from the conversation around data accessibility. The following considerations and recommendation are from the research that has been conducted in the data visualization space for an audience that has IDD. Historically individuals with IDD have been pushed to the side of conversations regarding their own data as well as data around them that they would like to interact with. The main component of data visualization that data scientists consider when creating visualizations for those with cognitive disabilities or to be accessible to those with cognitive disabilities are how the data are presented. For cognitive disabilities when creating visualizations there are some specific considerations to keep in mind.\n\n\nRethinking how we can present data to those with IDD is at the forefront of creating data that is more accessible for those with IDD. An emerging form of data science presentation is through the use of touch. Data scientists and artists are pairing up in order to create interactive displays and sculptures that present data. These data can be shown through the use of shapes, colors, and scaled sizing in an approachable format that prioritizes a major theme for each sculpture or installation(Corona 2019). Data presented in this way can allow individuals who might be unable to comprehend a visualization with many individual pieces the opportunity to interact with the data and understand the takeaway being shown.\nAn additional consideration when designing a data visualization is the type of figure that will be used. Research done at the University of Chapel Hill has revealed that pie charts are not accessible to those with IDD(Wu and Albers Szafir 2023). Thus, researchers and data scientists should consider if there are other visualizations that can be utilized.\nFurthermore, something that data scientists should always consider is how to emphasize the main takeaway from their visualization. Data scientists should consider the level of visual complexity they are presenting and potentially isolate different takeaways with different figures. Clearly presenting the key finding of a figure can allow everybody to better understand a data visualization.\n\n\n\nWhen creating a data visualization and having the potential for an audience that has cognitive disabilities there are some design components that can be added or highlighted which will in general increase comprehension.\n\n\nData scientists should ensure that labeling is intuitive, As an example, if data is being displayed regarding money, an easy to add label would be the $(wu2023a?).\n\n\n\nIt is important to ensure that data presented in visualizations has clear start and end points(wu2023a?). Indication of where data is beginning and where data is ending on a visualization such as axes marks allows an audience member to definitively know where they should stop visually processing.\n\n\n\nInformative titles can also be used to improve comprehension of a visualization(Knaflic Nussbaumer 2018). A title that includes the key takeaway from a visualization allows users to look for something in the data that supports the title which can increase comprehension.\n\n\n\n\n\nVisual disability is a large and varied field. It is estimated that roughly eight percent of the U.S. population has blindness or a visual impairment that affects their daily life. Visual impairments is considered an umbrella term and is more concretely defined as vision that is unable to be corrected through the use of aids (glasses, or contacts) or surgery(Institute, n.d.). It is important to consider how data visualizations are created in order to maintain accessibility for users who have visual impairments.\n\n\nAn emerging form of data representation is through the use of sounds. Sonified data uses non-speech sounds to explore and describe data. Data scientists can import data and create graphics through programs like the SAS Graphics Accelerator. The SAS Graphics accelerator enables scientists and the audiences they are trying to reach to access these data independently and create a framework they can succeed in.\n\n\nData scientists can add alternative text (commonly referred to as Alt Text) to data visualizations. Alternative texts are often labeled with figure titles which provide very little information to someone who cannot clearly see the visualization. Alternative text should instead be thought of as a typed explanation of the figure. By providing more details in the alternative text adaptive technologies, like screen readers, can read the figure details to the user. Important things to include in the alt text would be the title, key trends, and a potential link to data in a readable format like a csv file(Nussbaumer Knaflic 2018). An additional consideration is that screen readers do not allow the user to alter or skip what is being read so it is important to ensure that the text you include is well thought out but a reasonable length.\n\n\n\nFigures and visualizations can also include a link or a sound-bite that walks a user through the figure. The use of auditory explanations allows users to see and explore data through the creators eyes as well as potentially receive a more-detailed explanation than the alternative text.\n\n\n\n\n\n\nA potentially more common visual impairment is colorblindness. There are packages within R and Python that are specifically designed and built for people with color blindness(Ou 2021). Using colorblind-friendly color schemes is good practice to increase the accessibility of any chart or figure that is using color to differentiate data. Some examples of color-blind friendly palettes are the following:\n\nmunsell\nviridis and RColorBrewer\ndichromat\ncolorblindr\nshades\nggsci\n\n\n\n\nHigh contrast elements also allow for increased readability for those with visual impairments(nussbaumerknaflic2018?). The contrast between colors of elements and the sizes of these elements can affect the visual ease of use for users. There are multiple tools that data scientists can utilize to check the contrast of their images and even design a palette they would like to use.\n\n\n\nA barrier to users with low vision can be legends(nussbaumerknaflic2018?). Legends can typically include smaller text that is positioned outside of where the bulk of data is shown through the figure. This can create a disconnect between the data shown and any qualifiers assigned to it like scale, or type. A possible solution for this is direct labeling. Direct labeling can be labels next to data or within the data visualization.\n\n\n\n\n\nWe have covered numerous strategies in order to make data visualizations more accessible. Now depending on the goal of a researcher doing every one of these strategies could be an impossible task considering timelines and budgets. However, it is important for data scientists to think critically about who their audience is and if some of the strategies discussed can be applied. Some additions and considerations like, incorporating alternative text that is clear and concise and high contrast color palettes take minimal time and are often pre-built into some packages.\nAs the field of data science continues to grow and be at the forefront of scientific communication, especially environmental scientific communication, we as data scientists need to keep in mind how our visualizations and findings are received."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#background",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#background",
    "title": "Data Accessibility",
    "section": "",
    "text": "The topic I would like to focus on for this blog post centers around data justice and data accessibility to individuals that have disabilities and how our visualizations and presentation of data matter.\nOne of the key roles that data science plays is in the effective communication of data. Data visualizations are the main vehicle that allows data scientists to effectively convey findings and reach a broader audience. A data visualization is “the representation of data through use of common graphics” (IBM, n.d.). However, in order to promote equitable access to data and its findings it is important to consider that not all people are able to process data the same way. In the United States alone it is estimated that roughly twenty-five percent of the adult population has some sort of disability (Kahn 2023). Disabilities can range from cognitive, auditory, visual, and other levels of ability. And each of these disabilities can affect how someone is able to intake data and data visualizations.\n\n\n\n\n\nThe following blog post aims to explore barriers for disabled persons to data access and actions data scientists can take in order to maintain data justice through data access and accessibility for all.\nThe main disabilities I will be exploring in the following sections are cognitive, and visual."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#cognitive",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#cognitive",
    "title": "Data Accessibility",
    "section": "",
    "text": "Cognitive disability, which can also be commonly referred to as Intellectual and Developmental Disabilities (IDD), has been a group of individuals often absent from the conversation around data accessibility. The following considerations and recommendation are from the research that has been conducted in the data visualization space for an audience that has IDD. Historically individuals with IDD have been pushed to the side of conversations regarding their own data as well as data around them that they would like to interact with. The main component of data visualization that data scientists consider when creating visualizations for those with cognitive disabilities or to be accessible to those with cognitive disabilities are how the data are presented. For cognitive disabilities when creating visualizations there are some specific considerations to keep in mind.\n\n\nRethinking how we can present data to those with IDD is at the forefront of creating data that is more accessible for those with IDD. An emerging form of data science presentation is through the use of touch. Data scientists and artists are pairing up in order to create interactive displays and sculptures that present data. These data can be shown through the use of shapes, colors, and scaled sizing in an approachable format that prioritizes a major theme for each sculpture or installation(Corona 2019). Data presented in this way can allow individuals who might be unable to comprehend a visualization with many individual pieces the opportunity to interact with the data and understand the takeaway being shown.\nAn additional consideration when designing a data visualization is the type of figure that will be used. Research done at the University of Chapel Hill has revealed that pie charts are not accessible to those with IDD(Wu and Albers Szafir 2023). Thus, researchers and data scientists should consider if there are other visualizations that can be utilized.\nFurthermore, something that data scientists should always consider is how to emphasize the main takeaway from their visualization. Data scientists should consider the level of visual complexity they are presenting and potentially isolate different takeaways with different figures. Clearly presenting the key finding of a figure can allow everybody to better understand a data visualization.\n\n\n\nWhen creating a data visualization and having the potential for an audience that has cognitive disabilities there are some design components that can be added or highlighted which will in general increase comprehension.\n\n\nData scientists should ensure that labeling is intuitive, As an example, if data is being displayed regarding money, an easy to add label would be the $(wu2023a?).\n\n\n\nIt is important to ensure that data presented in visualizations has clear start and end points(wu2023a?). Indication of where data is beginning and where data is ending on a visualization such as axes marks allows an audience member to definitively know where they should stop visually processing.\n\n\n\nInformative titles can also be used to improve comprehension of a visualization(Knaflic Nussbaumer 2018). A title that includes the key takeaway from a visualization allows users to look for something in the data that supports the title which can increase comprehension."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#visual",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#visual",
    "title": "Data Accessibility",
    "section": "",
    "text": "Visual disability is a large and varied field. It is estimated that roughly eight percent of the U.S. population has blindness or a visual impairment that affects their daily life. Visual impairments is considered an umbrella term and is more concretely defined as vision that is unable to be corrected through the use of aids (glasses, or contacts) or surgery(Institute, n.d.). It is important to consider how data visualizations are created in order to maintain accessibility for users who have visual impairments.\n\n\nAn emerging form of data representation is through the use of sounds. Sonified data uses non-speech sounds to explore and describe data. Data scientists can import data and create graphics through programs like the SAS Graphics Accelerator. The SAS Graphics accelerator enables scientists and the audiences they are trying to reach to access these data independently and create a framework they can succeed in.\n\n\nData scientists can add alternative text (commonly referred to as Alt Text) to data visualizations. Alternative texts are often labeled with figure titles which provide very little information to someone who cannot clearly see the visualization. Alternative text should instead be thought of as a typed explanation of the figure. By providing more details in the alternative text adaptive technologies, like screen readers, can read the figure details to the user. Important things to include in the alt text would be the title, key trends, and a potential link to data in a readable format like a csv file(Nussbaumer Knaflic 2018). An additional consideration is that screen readers do not allow the user to alter or skip what is being read so it is important to ensure that the text you include is well thought out but a reasonable length.\n\n\n\nFigures and visualizations can also include a link or a sound-bite that walks a user through the figure. The use of auditory explanations allows users to see and explore data through the creators eyes as well as potentially receive a more-detailed explanation than the alternative text.\n\n\n\n\n\n\nA potentially more common visual impairment is colorblindness. There are packages within R and Python that are specifically designed and built for people with color blindness(Ou 2021). Using colorblind-friendly color schemes is good practice to increase the accessibility of any chart or figure that is using color to differentiate data. Some examples of color-blind friendly palettes are the following:\n\nmunsell\nviridis and RColorBrewer\ndichromat\ncolorblindr\nshades\nggsci\n\n\n\n\nHigh contrast elements also allow for increased readability for those with visual impairments(nussbaumerknaflic2018?). The contrast between colors of elements and the sizes of these elements can affect the visual ease of use for users. There are multiple tools that data scientists can utilize to check the contrast of their images and even design a palette they would like to use.\n\n\n\nA barrier to users with low vision can be legends(nussbaumerknaflic2018?). Legends can typically include smaller text that is positioned outside of where the bulk of data is shown through the figure. This can create a disconnect between the data shown and any qualifiers assigned to it like scale, or type. A possible solution for this is direct labeling. Direct labeling can be labels next to data or within the data visualization."
  },
  {
    "objectID": "blog-posts/2023-11-26-data-accessibility/index.html#takeaway",
    "href": "blog-posts/2023-11-26-data-accessibility/index.html#takeaway",
    "title": "Data Accessibility",
    "section": "",
    "text": "We have covered numerous strategies in order to make data visualizations more accessible. Now depending on the goal of a researcher doing every one of these strategies could be an impossible task considering timelines and budgets. However, it is important for data scientists to think critically about who their audience is and if some of the strategies discussed can be applied. Some additions and considerations like, incorporating alternative text that is clear and concise and high contrast color palettes take minimal time and are often pre-built into some packages.\nAs the field of data science continues to grow and be at the forefront of scientific communication, especially environmental scientific communication, we as data scientists need to keep in mind how our visualizations and findings are received."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Melissa Widas",
    "section": "",
    "text": "Melissa Widas earned a Bachelor’s of science degree from Montana State University in Environmental Sciences with a focus in land rehabilitation in 2022. During her time at Montana State, Melissa found a passion for data science through a research internship with the GeoSWIRL Lab. This opportunity resulted in the creation and management of a drought visualization tool for watersheds across the state of Montana. The development of this tool relied on the use of geographic information sciences and performed statistical analyses to highlight watershed change and risk of drought within watersheds. After experiencing a taste of data science, Melissa wanted to expand her knowledge to explore its effect on environmental practices. She is now pursuing a Master’s of Environmental Data Science at the Bren School at the University of California at Santa Barbara. Melissa is looking forward to using the power of data science and data visualization to help answer environmental questions regarding habitat restoration, remediation, and the efficacy of these practices to ameliorate the lands around her."
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html",
    "href": "blog-posts/2023-12-01-houston-outages/index.html",
    "title": "Houston, Texas Power Outages",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”[^1]\nIn the following exploration adapted from Dr. Ruth Oliver we will be:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nAnalysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will detect differences in night lights before and after the storm to identify areas that lost electric power."
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html#background",
    "href": "blog-posts/2023-12-01-houston-outages/index.html#background",
    "title": "Houston, Texas Power Outages",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”[^1]\nIn the following exploration adapted from Dr. Ruth Oliver we will be:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nAnalysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will detect differences in night lights before and after the storm to identify areas that lost electric power."
  },
  {
    "objectID": "blog-posts/2023-12-01-houston-outages/index.html#lets-start-coding",
    "href": "blog-posts/2023-12-01-houston-outages/index.html#lets-start-coding",
    "title": "Houston, Texas Power Outages",
    "section": "Let’s Start Coding",
    "text": "Let’s Start Coding\nThis section illustrates the code utilized to explore the effects of the storms.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)\nlibrary(raster)\nlibrary(ggspatial)\nlibrary(cowplot)\n\n\nLoading and Exploring our Data\nTo investigate the effects of the Houston, TX storms we will use raster data primarily. The following code block uses SQL and the stars package to load in the data needed for the project.\nFor our raster data we used NASA’s Worldview data for February 7, 2021 (before the power outage) and February 16, 2021 (during the power outage). These dates were selected as other days during the time frame had cloud cover that would have skewed our investigation.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Data was downloaded and prepped in advance and stored in the .gitignore.\n\n\nCode\n# read in night lights tiles\nnight1 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif')\nnight2 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif')\nnight3 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif')\nnight4 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif')\n\n# check class of night light tiles\nclass(night1)\n\n# combine tiles into stars object for 2021-02-07 and 2021-02-16\nnight_07 &lt;- st_mosaic(night1, night2)\nnight_16 &lt;- st_mosaic(night3, night4)\n\n# view 2021-02-07 raster\nplot(night_07, main = \"Raster of 2021-02-07\")\n\n\n\n\n\nNext we will minimize light emanating from major highways systems, we used geographic data from OpenStreetMap (OSM) from Geofabrik’s download sites. These data were downloaded and prepped in advance to contain a subset of highway and roads that intersect with the Houston metropolitan area. Data from Geofabrik’s download sites for residences in the Houston metropolitan area were also accessed.\n\n\nCode\n# creates the query to select motorways\nhighway_query &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# load highway data that fulfills query \nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = highway_query)\n\n# reproject highwyas to crs 3083\nhighways_reproj &lt;- st_transform(highways, crs = 3083)\n\n# creates the query to select residential buildings\nresidential_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# load buildings data that fulfills query \nresidential &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = residential_query)\n\n# reproject buildings to crs 3083\nresidential_reproj &lt;- st_transform(residential, crs = 3083)\n\n\nFor the final data set utilized, data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 from an ArcGIS file geodatabase was accessed. The metadata for each layer is available at ACS metadata.\n\n\nCode\n# load data that contains census tract geometries\ntexas_census &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer =  \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n# reproject census data to crs 3083\ntexas_census_reproj &lt;- st_transform(texas_census, crs = 3083)\n\n# load data that contains census tract data and income data\ntexas_income &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer =  \"X19_INCOME\")\n\n\nBlackout Mask\nTo find the change in night lights intensity (presumably) caused by the storm we will create a blackout mask. In order to do this we will be reclassifying the difference between the two rasters. We will be classifying any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout, and any location less than this will be classified to a NA\n\n\n\nCode\n# find the change in night light intensity between the two dates \nlight_difference &lt;- night_07-night_16\n\n# check class of difference data created \nclass(light_difference)\n\n# reclassifying raster to show blackouts above 200, and label areas that experienced an outage outage, and that didn't NA\nlight_difference_outage &lt;- cut(light_difference, c(200, Inf), labels = 'outage')\n\n#show where blackouts were experienced\nplot(light_difference_outage, main = \"Areas that Experienced a Blackout\")\n\n\n\n\n\nThe blackout mask will then be vectorized and fixed for any invalid geometries.\n\n\n\nCode\n# vectorize the blackout mask to make this a spatial feature\nblackouts &lt;- st_as_sf(light_difference_outage) %&gt;% \n  st_make_valid()\n\n# represents outages in houston above 200\nplot(blackouts, main = \"Houston, TX Metropolitan Areas that Experienced a Blackout\")\n\n\n\n\n\nThe vectorized mask was then cropped to our region of interest of Houston, TX with the following coordinates being utilized: (-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29). The blackout mask was then cropped to the size of Houston , TX and ultimately reprojected to EPSG:3083. The EPSG:3083 is also known as NAD83 / Texas Centric Albers Equal Area.\n\n\n\nCode\n# defining houston coordinates\nhouston &lt;- cbind(x = c(-96.5, -96.5, -94.5, -94.5, -96.5), \n                         y = c(29, 30.5, 30.5, 29, 29))\n\n#turn houston coordinates into polygon\nhouston &lt;- st_sfc(st_polygon(list(houston)), crs = 4326)\n\n# create a mask for the houston area \nhouston_mask &lt;- st_intersects(blackouts, houston, sparse = FALSE)\n\n# crop the blackout_mask to the houston region of interest \nhouston_blackouts &lt;- blackouts[houston_mask,]\n\n# reprojecting houston blackouts to Texas specific Datum\nhouston_blackouts_reproj &lt;- st_transform(houston_blackouts, crs = 3083)\n\n# plot houston blackouts \nplot(houston_blackouts_reproj, main = \"Blackouts in Houston\")\n\n\n\n\n\nHighways and Residences Selection\nWe then excluded highways, and created a 200 meter buffer to include buildings outside of 200 meters from a highway. Additionally we filtered to the residential buildings only within the blackout mask. These processes allow us to identify the number of homes impacted by the blackouts.\n\n\n\nCode\n# identify areas within 200m of all highways\nhighways_buffer &lt;- st_buffer(highways_reproj, dist = 200) %&gt;% \n  st_union()\n\n# find areas that had blackouts that are further than 200m \nblackouts_outside_highway &lt;- st_difference(houston_blackouts_reproj, highways_buffer)\n\n# blackouts outside of the highway buffer should be less than all of the houston blackouts\nnrow(blackouts_outside_highway)&lt;nrow(houston_blackouts_reproj)\n\n# dataframe where residential buildings are kept that are in houston outside of our highway buffer\nblackout_residential &lt;- residential_reproj[blackouts_outside_highway, , op = st_intersects]\n\n# count number of buildings in blackout areas \nnrow(blackout_residential)\n\n#counting the number of residential buildings that got hit\nprint(paste0(nrow(blackout_residential), ' residential buildings were affected by the blackouts'))\n\n\nSocioeconomic Effects\nTo explore if blackouts were correlated with socioeconomic factors like median income, we used census tract data. We used spatial joins to join the census tract data, and buildings that were and were not impacted by blackouts.\n\n\n\nCode\n# create data frame that contains median income data and geoID\ntexas_median_income &lt;- texas_income %&gt;% \n  dplyr::select(B19013e1, GEOID) %&gt;% \n  rename(median_income = B19013e1, GEOID_Data = GEOID)\n\n# join the income data to census tract geometries in Texas\ntexas_income_census &lt;- left_join(texas_census_reproj, texas_median_income, by = \"GEOID_Data\")\n\n# join the census tract data with residential buildings determined to be impacted by blackouts\ntexas_census_blackouts &lt;- st_filter(texas_income_census, blackout_residential) \n\n\nComparison of median income for census tracts impacted by blackouts and those that were not\nTo compare incomes of impacted census tracts and unimpacted census tracts a map of median income per census tract with the tracts that had blackouts outlined was created.\n\n\nCode\n#transforming houston boundary to Texas datum\nhouston_reproj &lt;- st_transform(houston, crs = 3083)\n\n# data frame that has all median income is in houston census tracts\nhouston_income_census &lt;- st_filter(texas_income_census, houston_reproj)\n\n# create visualization\nmedian_income_map &lt;- ggplot() +\n  geom_sf(data = houston_income_census, # plot the median income of all of houston\n          aes(fill = median_income)) +\n  scale_fill_viridis_c() +\n  geom_sf(data = texas_census_blackouts, aes(color = \"red\", # outline census tracts that experienced blackouts in red\n                                       fill = median_income)) +\n  scale_color_hue(labels = c(\"Blackout Census Tracts\")) + # alter legend titles\n  labs(fill='Median Income') +\n  theme_void() +\n  annotation_north_arrow( # add north arrow\n  height = unit(1.5, \"cm\"),\n  width = unit(1.5, \"cm\"),\n  pad_x = unit(7.5, \"cm\"),\n  pad_y = unit(0.15, \"cm\"),\n  rotation = NULL,\n  style = north_arrow_fancy_orienteering\n) +\n  annotation_scale( # add scale bar\n  plot_unit = NULL,\n  bar_cols = c(\"black\", \"white\"),\n  line_width = 1,\n  height = unit(0.15, \"cm\"),\n  pad_x = unit(2, \"cm\"),\n  pad_y = unit(0.15, \"cm\"),\n  text_pad = unit(0.15, \"cm\"),\n  text_cex = 0.7,\n  text_face = NULL,\n  text_family = \"\",\n  tick_height = 0.6\n) +\n  labs(title= \"Median Income of Census Tracts in Houston, TX\", color = \"\") # add map title\n  \nmedian_income_map\n\n\n\n\n\nTo further explore the impacts of blackouts on census tracts the distribution of income in impacted and unimpacted tracts was plotted.\n\n\n\nCode\n# find areas that did not have blackouts within the houston area \nunimpacted &lt;- houston_income_census[texas_census_blackouts, , op = st_difference]\n\n# create a histogram showing median income spread for homes in houston not impacted by blackouts \nunimpacted_figure &lt;- ggplot(data = unimpacted, aes(median_income)) +\n  geom_histogram(bins=50, fill = '#481467FF', color = 'white') +\n  labs(title = \"Unimpacted by Blackouts\", x = \"Median Income ($)\", y = \"Census Tracts\") +\n  theme_classic() +\n  ylim(0,105)\n  \n#unimpacted_figure\n\n# create a histogram showing median income spread for homes in houston impacted by blackouts\nimpacted_figure &lt;- ggplot(data = texas_census_blackouts, aes(median_income)) +\n  geom_histogram(bins=50, fill = '#481467FF', color = 'white') +\n  labs(title = \"Impacted by Blackouts\", x = \"Median Income ($)\", y = \"Census Tracts\") +\n  theme_classic() +\n  ylim(0,105)\n\n# position histograms side by side for easier comparison\nimpact_unimpact_plot &lt;- plot_grid(unimpacted_figure, impacted_figure, labels = c('', ''), label_size = 12)\n\nimpact_unimpact_plot\n\n\n\n\n\nThe median income of homes in Houston, TX affected and not affected by blackouts does not appear to have a significant difference. The blackouts do appear to have centered around the center of the city, but this area also has a wide variety of median incomes. Additionally, when comparing the median incomes of homes affected and not affected the distributions appear to be similar for the spread of wealth. Residents not impacted by the storm do appear to have slightly higher median incomes which could coincide with living farther outside the city center in the suburbs of Houston. A limitation of this analysis is that we did reduce the number of buildings counted that fell close to highways, which could be areas that have lower median incomes. Additionally, in exploring the power outages in Houston, TX we only compared the blackouts from the first two storms, without accounting for the third storm. Without accounting for the third storm the total number of outages that occurred is being underestimated."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]